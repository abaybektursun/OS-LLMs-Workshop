{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMDbuSHMre2YfpQPZzzwIbW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abaybektursun/OS-LLMs-Workshop/blob/master/OSS_LLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUY8HWU6f_wT",
        "outputId": "3e1080d5-a8f3-499e-ad3c-4928c2db488a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.0.191-py3-none-any.whl (993 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m993.7/993.7 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.10)\n",
            "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain)\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0.0,>=4.0.0 (from langchain)\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.5.7-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.4)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.22.4)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain)\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.7)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.27.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.3.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marshmallow-enum<2.0.0,>=1.5.1 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Collecting typing-inspect>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, multidict, marshmallow, frozenlist, async-timeout, yarl, typing-inspect, openapi-schema-pydantic, marshmallow-enum, aiosignal, dataclasses-json, aiohttp, langchain\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 dataclasses-json-0.5.7 frozenlist-1.3.3 langchain-0.0.191 marshmallow-3.19.0 marshmallow-enum-1.5.1 multidict-6.0.4 mypy-extensions-1.0.0 openapi-schema-pydantic-1.2.4 typing-inspect-0.9.0 yarl-1.9.2\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "has_gpu = False\n",
        "if torch.cuda.is_available():\n",
        "    install_cmd = \"CMAKE_ARGS=\\\"-DLLAMA_CUBLAS=on\\\" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python\"\n",
        "    has_gpu = True\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    install_cmd = \"CMAKE_ARGS=\\\"-DLLAMA_OPENBLAS=on\\\" FORCE_CMAKE=1 pip install llama-cpp-python\"\n",
        "    print(\"GPU is not available, using CPU\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12-FdN-B_Ady",
        "outputId": "58f4673e-10c8-4076-abe9-86be16ac843e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is available\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!{install_cmd}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6yhfyvDxeer",
        "outputId": "70523b46-c441-4164-db57-fad7e0166da2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.1.57.tar.gz (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting typing-extensions>=4.5.0 (from llama-cpp-python)\n",
            "  Downloading typing_extensions-4.6.3-py3-none-any.whl (31 kB)\n",
            "Collecting numpy>=1.20.0 (from llama-cpp-python)\n",
            "  Downloading numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.1.57-cp310-cp310-linux_x86_64.whl size=252308 sha256=369a68b8eb6aeea3e54f7ad8f49cb3ceff20de1230c11d7ecca4e37dfa900757\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/9b/55/22559358a9af8074053b6c39406f5c06f0d391c052f77ca0c2\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: typing-extensions, numpy, llama-cpp-python\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.22.4\n",
            "    Uninstalling numpy-1.22.4:\n",
            "      Successfully uninstalled numpy-1.22.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.24.3 which is incompatible.\n",
            "tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed llama-cpp-python-0.1.57 numpy-1.24.3 typing-extensions-4.6.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "available_models = {\n",
        "    \"stable-vicuna-13B.ggmlv3.q4_0.bin\":\n",
        "         {\n",
        "            \"url\":\"https://huggingface.co/TheBloke/stable-vicuna-13B-GGML/resolve/main/stable-vicuna-13B.ggmlv3.q4_0.bin\",\n",
        "            \"desc\": \"CarperAI StableVicuna\" + \\\n",
        "                     \"13 Billion Parameters, GGML, CPU Only, Q4\"\n",
        "         },\n",
        "    \"Wizard-Vicuna-13B-Uncensored.ggmlv3.q4_0.bin\": \n",
        "         {\n",
        "            \"url\":\"https://huggingface.co/TheBloke/Wizard-Vicuna-13B-Uncensored-GGML/resolve/main/Wizard-Vicuna-13B-Uncensored.ggmlv3.q4_0.bin\",\n",
        "            \"desc\": \"Wizard-Vicuna; 13 Billion Parameters, GGML, CPU ONLY, Q4\"\n",
        "         },\n",
        "    \"llama-7b.ggmlv3.q4_0.bin\": \n",
        "         {\n",
        "            \"url\":\"https://huggingface.co/TheBloke/LLaMa-7B-GGML/resolve/main/llama-7b.ggmlv3.q4_0.bin\",\n",
        "            \"desc\": \"Original LLaMa; 7 Billion Parameters, GGML, CPU/GPU, Q4\"\n",
        "         },\n",
        "    \"llama-deus-7b-v3.ggmlv3.q4_0.bin\":\n",
        "         {\n",
        "            \"url\":\"https://huggingface.co/TheBloke/llama-deus-7b-v3-GGML/resolve/main/llama-deus-7b-v3.ggmlv3.q4_0.bin\",\n",
        "            \"desc\": \"LLaMA DEUS V3; 7 Billion Parameters, GGML, CPU/GPU, Q4\"\n",
        "         }\n",
        "}"
      ],
      "metadata": {
        "id": "QrsypBvP0218"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for model in available_models:\n",
        "  print(model + ':')\n",
        "  print(\"\\t\" + available_models[model]['desc'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vi7_8uJBrVc8",
        "outputId": "ae92f827-efa1-4866-fc95-5c1eef4f3f40"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stable-vicuna-13B.ggmlv3.q4_0.bin:\n",
            "\tCarperAI StableVicuna13 Billion Parameters, GGML, CPU Only, Q4\n",
            "Wizard-Vicuna-13B-Uncensored.ggmlv3.q4_0.bin:\n",
            "\tWizard-Vicuna; 13 Billion Parameters, GGML, CPU ONLY, Q4\n",
            "llama-7b.ggmlv3.q4_0.bin:\n",
            "\tOriginal LLaMa; 7 Billion Parameters, GGML, CPU/GPU, Q4\n",
            "llama-deus-7b-v3.ggmlv3.q4_0.bin:\n",
            "\tLLaMA DEUS V3; 7 Billion Parameters, GGML, CPU/GPU, Q4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chosen_model = \"llama-deus-7b-v3.ggmlv3.q4_0.bin\""
      ],
      "metadata": {
        "id": "kuC5R2Uo1SvI"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget {available_models[chosen_model][\"url\"]}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PjOU_Oy14F2",
        "outputId": "d0f8bede-3c8e-404d-cc04-2140cb86ce18"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-06-06 22:06:10--  https://huggingface.co/TheBloke/llama-deus-7b-v3-GGML/resolve/main/llama-deus-7b-v3.ggmlv3.q4_0.bin\n",
            "Resolving huggingface.co (huggingface.co)... 13.249.85.16, 13.249.85.92, 13.249.85.127, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.249.85.16|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/57/0a/570a1297dec6fdfa2c36e42c33e01a303b00c63668ee56cb2f7482622a2b100d/2ac458de7cfcd73522f811d4742a72b6fe6578fed49cb288836ecadeea54e4cb?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama-deus-7b-v3.ggmlv3.q4_0.bin%3B+filename%3D%22llama-deus-7b-v3.ggmlv3.q4_0.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1686345873&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzU3LzBhLzU3MGExMjk3ZGVjNmZkZmEyYzM2ZTQyYzMzZTAxYTMwM2IwMGM2MzY2OGVlNTZjYjJmNzQ4MjYyMmEyYjEwMGQvMmFjNDU4ZGU3Y2ZjZDczNTIyZjgxMWQ0NzQyYTcyYjZmZTY1NzhmZWQ0OWNiMjg4ODM2ZWNhZGVlYTU0ZTRjYj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODYzNDU4NzN9fX1dfQ__&Signature=xkmNOEHRPzYYdhXdfD1sIuI-zdU9GtCKo0tgBc9Cc1NqCvGoUill-7XoKQ%7EmvZbvIstfvU02Tx6pmGy2YsMVXZlGWsdOiQZ4aHnvteJ6Yj9W0OhAt7hDwFhDEhamSLJYlZ86zIwLuEEp3u2-JVyFTfydP27yN1CnFY9J1bl66f%7EHDEZWLyfbG5kaZbVB5T6JKA4F9pMInHNrDnjcsC53ei8jqiaZXN8CPAa3Fr0QxRV3lZLIovOtyfy0SiqAH7ENfMBvaomWbMNZuY2%7ElW2mNy%7EtzGKR4ww%7EWCVYIfSX9qKSfjj-YNuuEf5CT8ZJfezkIHe8e1TsMCKv7fj4h878Tg__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-06-06 22:06:10--  https://cdn-lfs.huggingface.co/repos/57/0a/570a1297dec6fdfa2c36e42c33e01a303b00c63668ee56cb2f7482622a2b100d/2ac458de7cfcd73522f811d4742a72b6fe6578fed49cb288836ecadeea54e4cb?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama-deus-7b-v3.ggmlv3.q4_0.bin%3B+filename%3D%22llama-deus-7b-v3.ggmlv3.q4_0.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1686345873&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzU3LzBhLzU3MGExMjk3ZGVjNmZkZmEyYzM2ZTQyYzMzZTAxYTMwM2IwMGM2MzY2OGVlNTZjYjJmNzQ4MjYyMmEyYjEwMGQvMmFjNDU4ZGU3Y2ZjZDczNTIyZjgxMWQ0NzQyYTcyYjZmZTY1NzhmZWQ0OWNiMjg4ODM2ZWNhZGVlYTU0ZTRjYj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODYzNDU4NzN9fX1dfQ__&Signature=xkmNOEHRPzYYdhXdfD1sIuI-zdU9GtCKo0tgBc9Cc1NqCvGoUill-7XoKQ%7EmvZbvIstfvU02Tx6pmGy2YsMVXZlGWsdOiQZ4aHnvteJ6Yj9W0OhAt7hDwFhDEhamSLJYlZ86zIwLuEEp3u2-JVyFTfydP27yN1CnFY9J1bl66f%7EHDEZWLyfbG5kaZbVB5T6JKA4F9pMInHNrDnjcsC53ei8jqiaZXN8CPAa3Fr0QxRV3lZLIovOtyfy0SiqAH7ENfMBvaomWbMNZuY2%7ElW2mNy%7EtzGKR4ww%7EWCVYIfSX9qKSfjj-YNuuEf5CT8ZJfezkIHe8e1TsMCKv7fj4h878Tg__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 13.249.85.116, 13.249.85.12, 13.249.85.23, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|13.249.85.116|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3791725184 (3.5G) [application/octet-stream]\n",
            "Saving to: ‘llama-deus-7b-v3.ggmlv3.q4_0.bin’\n",
            "\n",
            "llama-deus-7b-v3.gg 100%[===================>]   3.53G  77.8MB/s    in 44s     \n",
            "\n",
            "2023-06-06 22:06:55 (81.9 MB/s) - ‘llama-deus-7b-v3.ggmlv3.q4_0.bin’ saved [3791725184/3791725184]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import LlamaCpp\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
      ],
      "metadata": {
        "id": "Mo3mjy0VgSJJ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's work this out in a step by step way to be sure we have the right answer.\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
      ],
      "metadata": {
        "id": "08ACOqXN1PBP"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Callbacks support token-wise streaming\n",
        "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
        "# Verbose is required to pass to the callback manager"
      ],
      "metadata": {
        "id": "jMKMnBmYgeS9"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if has_gpu:\n",
        "    llm = LlamaCpp(\n",
        "        model_path=chosen_model,\n",
        "        n_gpu_layers=1, n_batch=1,\n",
        "        callback_manager=callback_manager, \n",
        "        verbose=True\n",
        "    )\n",
        "else:\n",
        "    llm = LlamaCpp(\n",
        "        model_path=chosen_model, \n",
        "        callback_manager=callback_manager,\n",
        "        verbose=True\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJFO0XWTgiiW",
        "outputId": "d8e0b974-0802-4a53-a8d1-d4d769e4717c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain = LLMChain(prompt=prompt, llm=llm)"
      ],
      "metadata": {
        "id": "225jutiEhC5a"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What do to if I am attacked by a bear?\"\n",
        "\n",
        "llm_chain.run(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "udWJLqNMyVat",
        "outputId": "3edc781d-de38-4215-e1d7-7623099b3ee4"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Here are some steps you can take if you find yourself being attacked by a bear: \n",
            "\n",
            "1. Stay calm and don't panic, as this is what triggers most human reactions from animals.\n",
            "2. Don’t make any sudden movements or loud noises that might startle the bear or cause it to feel threatened.\n",
            "3. Make eye contact with the bear and show that you are not afraid by standing tall and firmly.\n",
            "4. If possible, try to look calm and non-threatening while keeping a safe distance from the bear. \n",
            "5. If the bear charges at you, defend yourself using any means available like rocks, sticks, or your hands. Aim for the bear's face or throat, and hit it hard.\n",
            "6. Once the bear has stopped moving, back away slowly while keeping an eye on the bear. Avoid turning your back on the bear and always keep it in sight. \n",
            "7. If the bear continues to approach you, throw something at it (such as rocks or logs) to distract it and give yourself time to run away.\n",
            "8. Once you are safe, leave the area quickly but slowly so that the bear doesn't"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Here are some steps you can take if you find yourself being attacked by a bear: \\n\\n1. Stay calm and don't panic, as this is what triggers most human reactions from animals.\\n2. Don’t make any sudden movements or loud noises that might startle the bear or cause it to feel threatened.\\n3. Make eye contact with the bear and show that you are not afraid by standing tall and firmly.\\n4. If possible, try to look calm and non-threatening while keeping a safe distance from the bear. \\n5. If the bear charges at you, defend yourself using any means available like rocks, sticks, or your hands. Aim for the bear's face or throat, and hit it hard.\\n6. Once the bear has stopped moving, back away slowly while keeping an eye on the bear. Avoid turning your back on the bear and always keep it in sight. \\n7. If the bear continues to approach you, throw something at it (such as rocks or logs) to distract it and give yourself time to run away.\\n8. Once you are safe, leave the area quickly but slowly so that the bear doesn't\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gRu71K9X1Gp2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}